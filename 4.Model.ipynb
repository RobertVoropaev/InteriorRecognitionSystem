{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разработка системы распознавания предметов интерьера в потоковом видео\n",
    "## Часть 4: Нейронная сеть для сегментации "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class def_config:\n",
    "\n",
    "    main_data_dir = \"data/ADE20K_encoded/\"\n",
    "    callbacks_dir = \"callbacks/\"\n",
    "\n",
    "    img_shape = 256\n",
    "    classes_num = 31\n",
    "\n",
    "    batch_size = 4\n",
    "    epoch_num = 2\n",
    "    train_coef = 1\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    last_activation = \"sigmoid\"\n",
    "    loss_function = \"categorical_crossentropy\"\n",
    "\n",
    "    gpu_memory_limit = 0.8\n",
    "    cpu_threads_num = 4\n",
    "\n",
    "    callbacks_monitor = \"val_jaccard_coef\"\n",
    "    callbacks_data_format = \"%m.%d_%H-%M\"\n",
    "    file_name = \"DefName\"\n",
    "    \n",
    "    is_load = False\n",
    "    argparse_is_on = False\n",
    "    \n",
    "    \n",
    "args = def_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rv/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "# Base\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Activation, Input\n",
    "from keras.layers import Conv2D, MaxPool2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import Dropout, BatchNormalization, Concatenate\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "# Preprocessing\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Seed\n",
    "seed = 99\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3389050060\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7426871812363537824\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=args.cpu_threads_num,\n",
    "                        inter_op_parallelism_threads=args.cpu_threads_num)\n",
    "config.gpu_options.per_process_gpu_memory_fraction = args.gpu_memory_limit\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "# GPU check list\n",
    "GPU_list = [x for x in device_lib.list_local_devices() \n",
    "            if x.device_type == 'GPU' or x.device_type == \"GPU\"]\n",
    "print(GPU_list)\n",
    "\n",
    "if not tf.test.is_gpu_available():\n",
    "    raise OSError(\"GPU not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Настройки сегментации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим папки с изображениями и масками для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data_dir = args.main_data_dir\n",
    "\n",
    "train_dir = main_data_dir + \"train/\"\n",
    "val_dir = main_data_dir + \"val/\"\n",
    "\n",
    "img_train_dir = train_dir + \"img/\"\n",
    "mask_train_dir = train_dir + \"mask/\"\n",
    "\n",
    "img_val_dir = val_dir + \"img/\"\n",
    "mask_val_dir = val_dir + \"mask/\"\n",
    "\n",
    "# Callbacks\n",
    "\n",
    "if def_config.argparse_is_on:\n",
    "    file_name = sys.argv[0].split(\".\")[-2]\n",
    "else:\n",
    "    file_name = def_config.file_name\n",
    "callbacks_dir = args.callbacks_dir\n",
    "\n",
    "try:\n",
    "    os.mkdir(callbacks_dir)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "callbacks_dir_name = file_name + now.strftime(\"_\" + def_config.callbacks_data_format) + \"/\"\n",
    "\n",
    "callbacks_full_dir = callbacks_dir + callbacks_dir_name\n",
    "try:\n",
    "    os.mkdir(callbacks_full_dir)\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер обучающей и валидационной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6522\n",
      "Val size: 644\n"
     ]
    }
   ],
   "source": [
    "train_size = len(os.listdir(path=train_dir + \"img/\"))\n",
    "val_size = len(os.listdir(path=val_dir + \"img/\"))\n",
    "print(\"Train size: \" + str(train_size))\n",
    "print(\"Val size: \" + str(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроки нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = args.img_shape\n",
    "batch_size = args.batch_size\n",
    "classes_num = args.classes_num\n",
    "\n",
    "epoch_num = args.epoch_num\n",
    "train_coef = args.train_coef\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "loss_function = args.loss_function\n",
    "last_activation = args.last_activation\n",
    "\n",
    "is_load = args.is_load\n",
    "if is_load:\n",
    "    weight_path = args.weight_path\n",
    "else:\n",
    "    weight_path = None\n",
    "\n",
    "\n",
    "with open(callbacks_dir + callbacks_dir_name + \"/\" + \"config.txt\", \"w\") as f:\n",
    "    if def_config.argparse_is_on:\n",
    "        args_str = str(args).lstrip(\"Namespace(\").rstrip(')')\n",
    "        args_arr = args_str.split(\", \")\n",
    "        f.write(\"\\n\".join(args_arr))\n",
    "    else:\n",
    "        f.write(\"Def_config\\n\")\n",
    "\n",
    "callbacks_monitor = args.callbacks_monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициент Дайса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return ((2. * intersection + smooth) / \n",
    "            (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициент Джакарда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return ((intersection + smooth) / \n",
    "            (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loss-функции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def jaccard_loss(y_true, y_pred):\n",
    "    return 1 - jaccard_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Генератор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(img_dir, mask_dir, classes_num, batch_size):\n",
    "    img_folder = img_dir\n",
    "    mask_folder = mask_dir\n",
    "\n",
    "    img_list = os.listdir(img_folder)\n",
    "    random.shuffle(img_list)\n",
    "    img_dir_size = len(img_list)\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "        img_list[i] = img_list[i].split(\".\")[0]  # отделяем имя от формата\n",
    "\n",
    "    counter = 0\n",
    "    while (True):\n",
    "        img = np.zeros((batch_size, img_shape, img_shape, 3)).astype('float')\n",
    "        mask = np.zeros((batch_size, img_shape, img_shape, classes_num)).astype(\"uint8\")\n",
    "\n",
    "        for i in range(counter, counter + batch_size):\n",
    "            train_img = cv2.imread(img_folder + '/' + img_list[i] + \".jpg\") / 255.\n",
    "            train_img = cv2.resize(train_img, (img_shape, img_shape))\n",
    "\n",
    "            img[i - counter] = train_img\n",
    "\n",
    "            train_mask = cv2.imread(mask_folder + '/' + img_list[i] + \".png\", cv2.IMREAD_GRAYSCALE)\n",
    "            train_mask = cv2.resize(train_mask, (img_shape, img_shape), interpolation=cv2.INTER_NEAREST)\n",
    "            train_mask = train_mask.reshape(img_shape, img_shape, 1)\n",
    "            train_mask = to_categorical(train_mask, num_classes=classes_num)\n",
    "\n",
    "            mask[i - counter] = train_mask\n",
    "\n",
    "        counter += batch_size\n",
    "\n",
    "        if counter + batch_size >= img_dir_size:\n",
    "            counter = 0\n",
    "            random.shuffle(img_list)\n",
    "\n",
    "        yield img, mask\n",
    "\n",
    "\n",
    "train_gen = data_gen(img_train_dir, mask_train_dir, classes_num=classes_num, batch_size=batch_size)\n",
    "val_gen = data_gen(img_val_dir, mask_val_dir, classes_num=classes_num, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(img_shape, classes_num, last_activation):\n",
    "    model_name = \"Unet0_model\"\n",
    "    \n",
    "    block0_input = Input(shape=(img_shape, img_shape, 3))\n",
    "\n",
    "    block1_conv1 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(block0_input)\n",
    "    block1_conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(block1_conv1)\n",
    "    block1_conv3 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(block1_conv2)\n",
    "    block1_pool1 = MaxPool2D(2)(block1_conv3)\n",
    "\n",
    "    block2_conv1 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(block1_pool1)\n",
    "    block2_conv2 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(block2_conv1)\n",
    "    block2_conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(block2_conv2)\n",
    "    block2_pool1 = MaxPool2D(2)(block2_conv3)\n",
    "\n",
    "    block3_conv1 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(block2_pool1)\n",
    "    block3_conv2 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(block3_conv1)\n",
    "    block3_conv3 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(block3_conv2)\n",
    "    block3_pool1 = MaxPool2D(2)(block3_conv3)\n",
    "\n",
    "    block4_conv1 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(block3_pool1)\n",
    "    block4_conv2 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(block4_conv1)\n",
    "    block4_conv3 = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(block4_conv2)\n",
    "    block4_upsa1 = UpSampling2D(2, interpolation=\"bilinear\")(block4_conv3)\n",
    "\n",
    "    block5_conc1 = Concatenate()([block3_conv3, block4_upsa1])\n",
    "    block5_conv1 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(block5_conc1)\n",
    "    block5_conv2 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(block5_conv1)\n",
    "    block5_conv3 = Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(block5_conv2)\n",
    "    block5_upsa1 = UpSampling2D(2, interpolation=\"bilinear\")(block5_conv3)\n",
    "\n",
    "    block6_conc1 = Concatenate()([block2_conv3, block5_upsa1])\n",
    "    block6_conv1 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(block6_conc1)\n",
    "    block6_conv2 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(block6_conv1)\n",
    "    block6_conv3 = Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(block6_conv2)\n",
    "    block6_upsa1 = UpSampling2D(2, interpolation=\"bilinear\")(block6_conv3)\n",
    "\n",
    "    block7_conc1 = Concatenate()([block1_conv3, block6_upsa1])\n",
    "    block7_conv1 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(block7_conc1)\n",
    "    block7_conv2 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(block7_conv1)\n",
    "    block7_conv3 = Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(block7_conv2)\n",
    "\n",
    "    block8_output = Conv2D(classes_num, (1, 1), padding=\"same\", activation=last_activation)(block7_conv3)\n",
    "\n",
    "    return Model(inputs=block0_input, outputs=block8_output), model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(dir_path, callbacks_monitor):\n",
    "    # лучшие веса\n",
    "    best_w = ModelCheckpoint(dir_path + \"best_w.h5\",\n",
    "                             monitor=callbacks_monitor,\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='auto',\n",
    "                             period=1\n",
    "                             )\n",
    "\n",
    "    # последние веса\n",
    "    last_w = ModelCheckpoint(dir_path + \"last_w.h5\",\n",
    "                             monitor=callbacks_monitor,\n",
    "                             verbose=0,\n",
    "                             save_best_only=False,\n",
    "                             save_weights_only=True,\n",
    "                             mode='auto',\n",
    "                             period=1\n",
    "                             )\n",
    "\n",
    "    # сохраняет историю обучения\n",
    "    logger = CSVLogger(dir_path + \"logger.csv\",\n",
    "                       append=False)\n",
    "\n",
    "    return [best_w, last_w, logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rv/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if last_activation != 'sigmoid' and last_activation != 'softmax':\n",
    "    raise ValueError(\"Incorrect last activation :\" + last_activation)\n",
    "\n",
    "model, model_name = get_model(None, classes_num, last_activation)\n",
    "\n",
    "plot_model(model=model, to_file=callbacks_full_dir + model_name + \".png\", show_shapes=True, dpi=200)\n",
    "\n",
    "if is_load:\n",
    "    if not weight_path:\n",
    "        raise ValueError(\"Don't load weight_path\")\n",
    "    model.load_weights(weight_path)\n",
    "\n",
    "if loss_function == 'categorical_crossentropy':\n",
    "    pass\n",
    "elif loss_function == 'dice_loss':\n",
    "    loss_function = dice_loss\n",
    "elif loss_function == 'jaccard_loss':\n",
    "    loss_function = jaccard_loss\n",
    "else:\n",
    "    raise ValueError(\"Incorrect loss function :\" + loss_function)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "              loss=loss_function,\n",
    "              metrics=[\"accuracy\", dice_coef, jaccard_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rv/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/rv/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[metrics/dice_coef/Identity/_429]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c4c9d285d079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks_full_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks_monitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[metrics/dice_coef/Identity/_429]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_gen,\n",
    "                    epochs=epoch_num,\n",
    "                    steps_per_epoch=int(train_coef * train_size) // batch_size,\n",
    "                    validation_data=val_gen,\n",
    "                    validation_steps=val_size // batch_size,\n",
    "                    verbose=1,\n",
    "                    callbacks=get_callbacks(callbacks_full_dir, callbacks_monitor)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
